# -*- coding: utf-8 -*-
"""
ë‰´ìŠ¤ ë¶„ì„ ëª¨ë“ˆ (ê°œì„  ë²„ì „)
"""
import json
import requests
import uuid
from bs4 import BeautifulSoup
import os
from dotenv import load_dotenv
import streamlit as st
from datetime import datetime

load_dotenv()


def debug_environment():
    """í™˜ê²½ ë³€ìˆ˜ ë° ì„¤ì • ë””ë²„ê·¸"""
    print("=" * 50)
    print("í™˜ê²½ ë””ë²„ê¹…")
    print("=" * 50)

    api_key = os.getenv('CLOVA_API_KEY')
    print(f"API í‚¤ ì¡´ì¬: {'âœ…' if api_key else 'âŒ'}")

    if api_key:
        print(f"API í‚¤ ê¸¸ì´: {len(api_key)}")
        print(f"API í‚¤ ì‹œì‘: {api_key[:15]}...")

    # Streamlit secrets í™•ì¸ (ì¡°ìš©íˆ)
    try:
        import streamlit as st
        if hasattr(st, 'secrets') and hasattr(st.secrets, 'get'):
            secrets_key = st.secrets.get('CLOVA_API_KEY')
            if secrets_key and not api_key:
                api_key = secrets_key
                print("Streamlit secretsì—ì„œ API í‚¤ ì‚¬ìš©")
    except:
        pass  # ì—ëŸ¬ ë©”ì‹œì§€ ì¶œë ¥í•˜ì§€ ì•ŠìŒ

    return api_key


def extract_article_content(url):
    """ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, timeout=15, headers=headers)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')

        # í™•ì¥ëœ ë³¸ë¬¸ ì„ íƒì
        content_selectors = [
            '.view_con_t', '.article_content', '.news_content', '.content',
            '.article-content', '.post-content', 'article', '.entry-content',
            '.article-body', '.news-body', '.post-body', '.article_txt',
            '#articleText', '.article_view', '.news_view', '.view_text'
        ]

        article_text = None
        for selector in content_selectors:
            article_content = soup.select_one(selector)
            if article_content:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in article_content(['script', 'style', 'iframe', 'nav', 'footer']):
                    tag.decompose()

                article_text = article_content.get_text(strip=True)
                if len(article_text) > 300:  # ìµœì†Œ ê¸¸ì´ í™•ì¸
                    print(f"ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µ: {selector} ({len(article_text)}ì)")
                    break

        # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
        if not article_text or len(article_text) < 300:
            all_text = soup.get_text(strip=True)
            # ë³¸ë¬¸ìœ¼ë¡œ ë³´ì´ëŠ” ë¶€ë¶„ ì¶”ì¶œ
            paragraphs = soup.find_all('p')
            if paragraphs:
                article_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
            else:
                article_text = all_text

        # JavaScript í•„ìš” ì‚¬ì´íŠ¸ ê°ì§€
        js_indicators = ['javascript', 'ìë°”ìŠ¤í¬ë¦½íŠ¸', 'enable', 'browser', 'disabled']
        if any(indicator in article_text.lower() for indicator in js_indicators) and len(article_text) < 1000:
            print("JavaScript í•„ìš” ì‚¬ì´íŠ¸ë¡œ íŒë‹¨")
            return None

        # ê¸¸ì´ ì œí•œ ë° ì •ë¦¬
        if article_text:
            article_text = article_text[:8000]  # í† í° ì œí•œ ê³ ë ¤

            if len(article_text) > 300:
                print(f"ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {len(article_text)}ì")
                return article_text

        print(f"ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {len(article_text) if article_text else 0}ì")
        return None

    except Exception as e:
        print(f"ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def summarize_with_clova(content, investor_type, api_key):
    """HyperCLOVA Xë¡œ ë‰´ìŠ¤ ìš”ì•½ (ê°œì„ ëœ ë²„ì „)"""
    if not api_key:
        print("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None

    request_id = str(uuid.uuid4()).replace('-', '')

    # ë‹¤ì–‘í•œ í—¤ë” ì„¤ì • ì‹œë„
    headers_list = [
        {
            'Authorization': api_key,
            'X-NCP-CLOVASTUDIO-REQUEST-ID': request_id,
            'Content-Type': 'application/json; charset=utf-8',
            'Accept': 'text/event-stream'
        },
        {
            'Authorization': f'Bearer {api_key}',
            'X-NCP-CLOVASTUDIO-REQUEST-ID': request_id,
            'Content-Type': 'application/json; charset=utf-8',
            'Accept': 'text/event-stream'
        }
    ]

    # íˆ¬ìì ìœ í˜•ë³„ ì„¸ë¶„í™”ëœ í”„ë¡¬í”„íŠ¸
    if investor_type == "MIRAE":
        system_prompt = """ë‹¹ì‹ ì€ ì—”í„°í…Œì¸ë¨¼íŠ¸ íˆ¬ì ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

MIRAEí˜• íˆ¬ììë¥¼ ìœ„í•´ ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ê°ê´€ì ì´ê³  ì²´ê³„ì ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.

**ìš”ì•½ ì›ì¹™:**
- ê¸°ì—…ì˜ ê³µì‹ ë°œí‘œ, ì‹¤ì , ì‚¬ì—… ê³„íš ë“± íŒ©íŠ¸ ì¤‘ì‹¬
- ê°ê´€ì ì´ê³  ì¤‘ë¦½ì ì¸ ì–´ì¡° ìœ ì§€
- íˆ¬ì íŒë‹¨ì— í•„ìš”í•œ êµ¬ì²´ì  ì •ë³´ í¬í•¨
- ì¶”ì¸¡ì´ë‚˜ ê°ì •ì  í‘œí˜„ ë°°ì œ
- 7-8ì¤„ì˜ ì²´ê³„ì ì¸ ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±

**í¬í•¨í•  ìš”ì†Œ:**
- í•µì‹¬ ì‚¬ì‹¤ê³¼ ë°°ê²½ ì •ë³´
- ê¸°ì—… ì¸¡ ê³µì‹ ì…ì¥ì´ë‚˜ ëŒ€ì‘
- ì¬ë¬´ì /ì‚¬ì—…ì  ì˜í–¥ ê°€ëŠ¥ì„±
- ê´€ë ¨ ê·œì •ì´ë‚˜ ì—…ê³„ ë™í–¥"""

    else:  # ASAPí˜•
        system_prompt = """ë‹¹ì‹ ì€ ì—”í„°í…Œì¸ë¨¼íŠ¸ íˆ¬ì ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ASAPí˜• íˆ¬ììë¥¼ ìœ„í•´ ë‹¤ìŒ ë‰´ìŠ¤ì˜ í•µì‹¬ì„ ë¹ ë¥´ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•´ì£¼ì„¸ìš”.

**ìš”ì•½ ì›ì¹™:**
- ì¦‰ê°ì ì¸ ì‹œì¥ ë°˜ì‘ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆëŠ” ìš”ì†Œ ìš°ì„ 
- íŒ¬ë¤ê³¼ ì—¬ë¡ ì˜ ë°˜ì‘ì„ ê³ ë ¤í•œ ê´€ì 
- ê°„ê²°í•˜ê³  ëª…í™•í•œ 4-5ì¤„ êµ¬ì„±
- ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ

**í¬í•¨í•  ìš”ì†Œ:**
- ì´ìŠˆì˜ í•µì‹¬ ë‚´ìš©
- ì˜ˆìƒë˜ëŠ” íŒ¬ë¤/ëŒ€ì¤‘ ë°˜ì‘
- ë‹¨ê¸°ì  ì£¼ê°€ ì˜í–¥ ìš”ì¸
- ì¦‰ê°ì  ëŒ€ì‘ì´ í•„ìš”í•œ ë¶€ë¶„"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"ë‰´ìŠ¤ ë‚´ìš©:\n\n{content[:6000]}"}  # ê¸¸ì´ ì œí•œ
    ]

    request_data = {
        'messages': messages,
        'topP': 0.6,
        'topK': 0,
        'maxTokens': 800 if investor_type == "MIRAE" else 400,
        'temperature': 0.1,
        'repetitionPenalty': 1.2,
        'stop': [],
        'includeAiFilters': True,
        'seed': 0
    }

    # ì—¬ëŸ¬ í—¤ë”ë¡œ ì‹œë„
    for i, headers in enumerate(headers_list):
        try:
            print(f"API í˜¸ì¶œ ì‹œë„ {i + 1}: {investor_type}í˜•")

            with requests.post(
                    'https://clovastudio.stream.ntruss.com/v3/chat-completions/HCX-005',
                    headers=headers,
                    json=request_data,
                    stream=True,
                    timeout=30
            ) as r:

                print(f"ì‘ë‹µ ìƒíƒœ: {r.status_code}")

                if r.status_code == 401:
                    print("ì¸ì¦ ì‹¤íŒ¨")
                    continue
                elif r.status_code != 200:
                    print(f"HTTP ì˜¤ë¥˜: {r.status_code}")
                    continue

                for line in r.iter_lines():
                    if line:
                        line_text = line.decode("utf-8")
                        if line_text.startswith('data:'):
                            try:
                                data = json.loads(line_text[5:])
                                if 'message' in data and data['message'].get('content'):
                                    if data.get('finishReason') == 'stop':
                                        summary = data['message']['content']
                                        print(f"ìš”ì•½ ìƒì„± ì„±ê³µ: {len(summary)}ì")
                                        return summary
                            except json.JSONDecodeError:
                                continue

        except Exception as e:
            print(f"API í˜¸ì¶œ ì‹¤íŒ¨ {i + 1}: {e}")
            continue

    return None


def get_fallback_summary(investor_type):
    """API ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ìš”ì•½"""
    if investor_type == "MIRAE":
        return """ë°ì´ì‹ìŠ¤ íŒ¬ë¯¸íŒ…ì—ì„œ ì‹œí–‰ëœ ê³¼ë„í•œ ë³¸ì¸í™•ì¸ ì ˆì°¨ê°€ ë…¼ë€ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ìš´ì˜ ì—…ì²´ëŠ” ì•”í‘œ ë°©ì§€ë¥¼ ëª©ì ìœ¼ë¡œ ìƒí™œê¸°ë¡ë¶€, ê¸ˆìœµì¸ì¦ì„œ ë“±ì˜ ê°œì¸ì •ë³´ ì œì¶œì„ ìš”êµ¬í–ˆìœ¼ë©°, ì´ì— ëŒ€í•´ íŒ¬ë“¤ì´ ê°•í•œ ë°˜ë°œì„ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 

ê°œì¸ì •ë³´ ë³´í˜¸ë²• ìœ„ë°˜ ê°€ëŠ¥ì„±ì´ ì œê¸°ë˜ê³  ìˆìœ¼ë©°, ë²•ì  ê²€í† ê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. ì—…ê³„ ì „ë¬¸ê°€ë“¤ì€ ì´ëŸ¬í•œ ê³¼ë„í•œ ìš”êµ¬ê°€ íŒ¬ë¤ ë¬¸í™”ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤ê³  ìš°ë ¤ë¥¼ í‘œëª…í–ˆìŠµë‹ˆë‹¤. 

í–¥í›„ ìœ ì‚¬í•œ ì´ë²¤íŠ¸ ìš´ì˜ ë°©ì‹ì— ëŒ€í•œ ì „ë°˜ì ì¸ ì¬ê²€í† ê°€ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì´ë©°, JYP ì¸¡ì€ ì¬ë°œ ë°©ì§€ë¥¼ ìœ„í•œ êµ¬ì²´ì ì¸ ëŒ€ì±… ë§ˆë ¨ì´ ìš”êµ¬ë˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤. ì´ë²ˆ ì‚¬ê±´ì€ ì•„í‹°ìŠ¤íŠ¸ì™€ íŒ¬ ê°„ì˜ ì‹ ë¢° ê´€ê³„ì—ë„ ì˜í–¥ì„ ë¯¸ì¹  ê°€ëŠ¥ì„±ì´ ë†’ì•„ ì¤‘ì¥ê¸°ì  ê´€ì ì—ì„œì˜ ëŒ€ì‘ì´ í•„ìš”í•©ë‹ˆë‹¤."""

    else:  # ASAPí˜•
        return """ë°ì´ì‹ìŠ¤ íŒ¬ë¯¸íŒ…ì—ì„œ ìƒí™œê¸°ë¡ë¶€, ê¸ˆìœµì¸ì¦ì„œ ì œì¶œì„ ìš”êµ¬í•˜ëŠ” ê³¼ë„í•œ ë³¸ì¸í™•ì¸ì´ ë…¼ë€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. íŒ¬ë“¤ì€ ê°œì¸ì •ë³´ ì¹¨í•´ë¼ë©° ê°•í•˜ê²Œ ë°˜ë°œí•˜ê³  ìˆìœ¼ë©°, ì†Œì…œë¯¸ë””ì–´ë¥¼ í†µí•´ ë¹„íŒ ì—¬ë¡ ì´ ê¸‰ì†íˆ í™•ì‚°ë˜ê³  ìˆëŠ” ìƒí™©ì…ë‹ˆë‹¤. 

ë²•ì  ë¬¸ì œ ì†Œì§€ë„ ì œê¸°ë˜ê³  ìˆì–´ ë‹¨ê¸°ì ìœ¼ë¡œ ë¶€ì •ì  ì—¬ë¡ ì´ ì§€ì†ë  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì´ë²ˆ ì‚¬ê±´ì€ íŒ¬ë¤ê³¼ ì•„í‹°ìŠ¤íŠ¸ ê°„ ì‹ ë¢° ê´€ê³„ì— ì¦‰ê°ì ì¸ íƒ€ê²©ì„ ì¤„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ë©°, ë¹ ë¥¸ ì‚¬ê³¼ì™€ ê°œì„ ì±… ë°œí‘œê°€ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤."""


def find_working_news(json_file, investor_type, max_tries=8):
    """JSONì—ì„œ ì‘ë™í•˜ëŠ” ë‰´ìŠ¤ ì°¾ì•„ì„œ ìš”ì•½ (ê°œì„  ë²„ì „)"""

    # í™˜ê²½ ë””ë²„ê¹…
    api_key = debug_environment()

    # JSON ì½ê¸°
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            news_data = json.load(f)
    except FileNotFoundError:
        print(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_file}")
        return {
            "title": "ë°ì´ì‹ìŠ¤ íŒ¬ë¯¸íŒ… ë³¸ì¸í™•ì¸ ë…¼ë€",
            "url": "#",
            "date": "2025-07-18",
            "summary": get_fallback_summary(investor_type),
            "investor_type": investor_type,
            "success": True,
            "source": "fallback"
        }

    if not news_data:
        return {"error": "ë‰´ìŠ¤ ë°ì´í„° ì—†ìŒ"}

    print(f"ğŸ” ì´ {len(news_data)}ê°œ ë‰´ìŠ¤ì—ì„œ ë¶„ì„ ì‹œì‘ ({investor_type}í˜•)")

    # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìˆœíšŒ
    for i, news in enumerate(news_data[:max_tries]):
        print(f"\n[{i + 1}/{min(max_tries, len(news_data))}] ì²˜ë¦¬ ì¤‘...")
        print(f"ì œëª©: {news['title'][:50]}...")

        # ë³¸ë¬¸ ì¶”ì¶œ
        article_text = extract_article_content(news['link'])

        if article_text:
            print(f"ë³¸ë¬¸ ê¸¸ì´: {len(article_text)}ì")

            # APIë¡œ ìš”ì•½ ì‹œë„
            if api_key:
                summary = summarize_with_clova(article_text, investor_type, api_key)

                if summary:
                    return {
                        "title": news['title'],
                        "url": news['link'],
                        "date": news.get('pub_date', '2025-07-18'),
                        "summary": summary,
                        "investor_type": investor_type,
                        "tried_count": i + 1,
                        "success": True,
                        "source": "api"
                    }
                else:
                    print("API ìš”ì•½ ì‹¤íŒ¨ - ë‹¤ìŒ ë‰´ìŠ¤ ì‹œë„")
            else:
                print("API í‚¤ ì—†ìŒ - ëŒ€ì²´ ìš”ì•½ ì‚¬ìš©")
                break
        else:
            print("ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - ë‹¤ìŒ ë‰´ìŠ¤ ì‹œë„")

    # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ ëŒ€ì²´ ìš”ì•½ ë°˜í™˜
    print("ëª¨ë“  ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨ - ëŒ€ì²´ ìš”ì•½ ì‚¬ìš©")
    return {
        "title": "ë°ì´ì‹ìŠ¤ íŒ¬ë¯¸íŒ… ë³¸ì¸í™•ì¸ ë…¼ë€",
        "url": "#",
        "date": "2025-07-18",
        "summary": get_fallback_summary(investor_type),
        "investor_type": investor_type,
        "success": True,
        "source": "fallback"
    }


def get_day6_news_summary(investor_type):
    """ë°ì´ì‹ìŠ¤ ë³¸ì¸í™•ì¸ ì´ìŠˆ ë‰´ìŠ¤ ìš”ì•½ (ê°œì„  ë²„ì „)"""
    json_file = "data/day6_news.json"

    # ìºì‹œ í™•ì¸
    cache_file = f"data/news_cache_{investor_type.lower()}.json"

    try:
        if os.path.exists(cache_file):
            with open(cache_file, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
                # ìºì‹œê°€ 1ì‹œê°„ ì´ë‚´ë¼ë©´ ì‚¬ìš©
                cache_time = datetime.fromisoformat(cached_data.get('timestamp', '2000-01-01'))
                if (datetime.now() - cache_time).seconds < 3600:
                    print(f"ìºì‹œëœ {investor_type}í˜• ë‰´ìŠ¤ ìš”ì•½ ì‚¬ìš©")
                    return cached_data['data']
    except:
        pass

    # ìƒˆë¡œ ìƒì„±
    result = find_working_news(json_file, investor_type)

    # ìºì‹œ ì €ì¥
    if result.get('success'):
        try:
            os.makedirs('data', exist_ok=True)
            cache_data = {
                'timestamp': datetime.now().isoformat(),
                'data': result
            }
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except:
            pass

    return result


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_news_analysis():
    """ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
    print("ë‰´ìŠ¤ ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    for investor_type in ["MIRAE", "ASAP"]:
        print(f"\n=== {investor_type}í˜• í…ŒìŠ¤íŠ¸ ===")
        result = get_day6_news_summary(investor_type)

        if result.get('success'):
            print(f"ì„±ê³µ: {result.get('source', 'unknown')}")
            print(f"ì œëª©: {result['title']}")
            print(f"ìš”ì•½ ê¸¸ì´: {len(result['summary'])}ì")
            print(f"ìš”ì•½ ë¯¸ë¦¬ë³´ê¸°: {result['summary'][:100]}...")
        else:
            print(f"ì‹¤íŒ¨: {result.get('error', 'unknown error')}")


if __name__ == "__main__":
    test_news_analysis()